{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image as img\n",
    "from imageio import mimsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_VGG19():\n",
    "    \n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    \n",
    "    vgg.trainable = False\n",
    "    \n",
    "    features_list = [tf.reshape(tf.transpose(layer.output, perm = [0,3,1,2]), [1,layer.output.shape[-1],-1])\n",
    "                 for layer in vgg.layers]\n",
    "    \n",
    "    extractor = tf.keras.Model(inputs = vgg.input, outputs = features_list)\n",
    "    \n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(Fp, Fc):\n",
    "    \n",
    "    return 0.5 * tf.reduce_sum(tf.square(Fp - Fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(F):\n",
    "    \n",
    "    return tf.matmul(F, F, transpose_b = True)\n",
    "\n",
    "def layer_gram_loss(Fp, Fc):\n",
    "\n",
    "    (_, nl, ml) = Fp.shape\n",
    "    \n",
    "    return tf.reduce_sum(tf.square(gram(Fp) - gram(Fc)))/(4 * nl**2 * ml**2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_loss(W, features_product, features_style):\n",
    "    \n",
    "    return tf.reduce_sum([\n",
    "        (1/len(W)) * layer_gram_loss(features_product[layernum], features_style[layernum])\n",
    "        for layernum in W\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(grad_image, \n",
    "              feature_model, \n",
    "              optimizer, \n",
    "              content_features, \n",
    "              style_features, \n",
    "              content_layer,\n",
    "              style_layers, \n",
    "              content_weight,\n",
    "              style_weight,\n",
    "              tvl_weight):\n",
    "    \n",
    "    assert(type(content_layer) == int)\n",
    "    assert(type(style_layers) == list)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        image_features = feature_model(preprocess_image(grad_image))\n",
    "        \n",
    "        loss = content_weight * content_loss(image_features[content_layer], content_features[content_layer]) + style_weight * gram_loss(style_layers, image_features, style_features)\n",
    "        loss += tvl_weight * tf.image.total_variation(grad_image)\n",
    "    \n",
    "    grads = tape.gradient(loss, grad_image)\n",
    "    optimizer.apply_gradients([(grads, grad_image)])\n",
    "    grad_image.assign(tf.clip_by_value(grad_image, 0., 1.))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    image = img.open(path)\n",
    "    image = image.resize((224,224))\n",
    "    x = np.array(image)\n",
    "    x = np.expand_dims(x, 0)\n",
    "    x = np.ndarray.astype(x, dtype = np.float32)\n",
    "    return x / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = image * 255\n",
    "    image = tf.keras.applications.vgg19.preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NST(content_path, \n",
    "        style_path, \n",
    "        epochs = 200,\n",
    "        steps_per_epoch = 5,\n",
    "        start_from_content = False,\n",
    "        content_layer = 18, \n",
    "        style_layers = [1,4,7,12,17], \n",
    "        content_weight = 1e4, \n",
    "        style_weight = 1e-2,\n",
    "        learning_rate = 0.02,\n",
    "        total_loss_variation_weight = 30,       \n",
    "       ):\n",
    "\n",
    "    \n",
    "    content_image = get_image(content_path)\n",
    "    style_image = get_image(style_path)\n",
    "    \n",
    "    extractor = prep_VGG19()\n",
    "\n",
    "    if not start_from_content:\n",
    "        grad_image = tf.Variable(np.ndarray.astype(np.random.rand(1,224,224,3), np.float32))\n",
    "    else:\n",
    "        grad_image = tf.Variable(content_image)\n",
    "\n",
    "    content_features, style_features = extractor(preprocess_image(content_image)), extractor(preprocess_image(style_image))\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate, beta_1=0.99, epsilon=1e-1)\n",
    "    \n",
    "    generated_images = []\n",
    "    \n",
    "    gif = np.zeros((epochs,224,224,3))\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_per_epoch):\n",
    "\n",
    "                loss = train_step(grad_image, \n",
    "                  extractor, \n",
    "                  optimizer, \n",
    "                  content_features, \n",
    "                  style_features, \n",
    "                  content_layer,\n",
    "                  style_layers, \n",
    "                  content_weight,\n",
    "                  style_weight,\n",
    "                  total_loss_variation_weight)\n",
    "\n",
    "            print('Epoch: {}, Loss: {}'.format(str(epoch), str(loss.numpy()[0])))\n",
    "            \n",
    "            gen_image = np.squeeze(grad_image.numpy())\n",
    "            \n",
    "            gif[epoch] = gen_image\n",
    "    finally:\n",
    "        return gif, gif[epoch - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3337373400000.0\n",
      "Epoch: 1, Loss: 2251820400000.0\n",
      "Epoch: 2, Loss: 1676206700000.0\n",
      "Epoch: 3, Loss: 1348027700000.0\n",
      "Epoch: 4, Loss: 1152686400000.0\n",
      "Epoch: 5, Loss: 1016218700000.0\n",
      "Epoch: 6, Loss: 913706000000.0\n",
      "Epoch: 7, Loss: 840514000000.0\n",
      "Epoch: 8, Loss: 779989600000.0\n",
      "Epoch: 9, Loss: 731284800000.0\n",
      "Epoch: 10, Loss: 687869400000.0\n",
      "Epoch: 11, Loss: 652885160000.0\n",
      "Epoch: 12, Loss: 621538250000.0\n",
      "Epoch: 13, Loss: 592897200000.0\n",
      "Epoch: 14, Loss: 566322900000.0\n",
      "Epoch: 15, Loss: 539295000000.0\n",
      "Epoch: 16, Loss: 515028680000.0\n",
      "Epoch: 17, Loss: 494045430000.0\n",
      "Epoch: 18, Loss: 474870900000.0\n",
      "Epoch: 19, Loss: 455578200000.0\n",
      "Epoch: 20, Loss: 438300050000.0\n",
      "Epoch: 21, Loss: 421900880000.0\n",
      "Epoch: 22, Loss: 406771100000.0\n",
      "Epoch: 23, Loss: 392561070000.0\n",
      "Epoch: 24, Loss: 379049150000.0\n",
      "Epoch: 25, Loss: 366642500000.0\n",
      "Epoch: 26, Loss: 354526660000.0\n",
      "Epoch: 27, Loss: 343143200000.0\n",
      "Epoch: 28, Loss: 332646220000.0\n",
      "Epoch: 29, Loss: 322161300000.0\n",
      "Epoch: 30, Loss: 312532730000.0\n",
      "Epoch: 31, Loss: 302927870000.0\n",
      "Epoch: 32, Loss: 293872930000.0\n",
      "Epoch: 33, Loss: 285430050000.0\n",
      "Epoch: 34, Loss: 277575600000.0\n",
      "Epoch: 35, Loss: 270195880000.0\n",
      "Epoch: 36, Loss: 262786760000.0\n",
      "Epoch: 37, Loss: 255595380000.0\n",
      "Epoch: 38, Loss: 248742600000.0\n",
      "Epoch: 39, Loss: 241655960000.0\n",
      "Epoch: 40, Loss: 235089220000.0\n",
      "Epoch: 41, Loss: 228623730000.0\n",
      "Epoch: 42, Loss: 222671440000.0\n",
      "Epoch: 43, Loss: 216550180000.0\n",
      "Epoch: 44, Loss: 210678800000.0\n",
      "Epoch: 45, Loss: 204778830000.0\n",
      "Epoch: 46, Loss: 199219150000.0\n",
      "Epoch: 47, Loss: 193860600000.0\n",
      "Epoch: 48, Loss: 188619260000.0\n",
      "Epoch: 49, Loss: 183304950000.0\n",
      "Epoch: 50, Loss: 178203230000.0\n",
      "Epoch: 51, Loss: 173650560000.0\n",
      "Epoch: 52, Loss: 168897900000.0\n",
      "Epoch: 53, Loss: 164664460000.0\n",
      "Epoch: 54, Loss: 160742210000.0\n",
      "Epoch: 55, Loss: 156786380000.0\n",
      "Epoch: 56, Loss: 152597640000.0\n",
      "Epoch: 57, Loss: 148649540000.0\n",
      "Epoch: 58, Loss: 144749460000.0\n",
      "Epoch: 59, Loss: 141198380000.0\n",
      "Epoch: 60, Loss: 137647850000.0\n",
      "Epoch: 61, Loss: 134296450000.0\n",
      "Epoch: 62, Loss: 131102380000.0\n",
      "Epoch: 63, Loss: 127956210000.0\n",
      "Epoch: 64, Loss: 124854500000.0\n",
      "Epoch: 65, Loss: 121739080000.0\n",
      "Epoch: 66, Loss: 118798655000.0\n",
      "Epoch: 67, Loss: 115998410000.0\n",
      "Epoch: 68, Loss: 113291120000.0\n",
      "Epoch: 69, Loss: 110465606000.0\n",
      "Epoch: 70, Loss: 107821560000.0\n",
      "Epoch: 71, Loss: 105128620000.0\n",
      "Epoch: 72, Loss: 102790685000.0\n",
      "Epoch: 73, Loss: 100449160000.0\n",
      "Epoch: 74, Loss: 98213410000.0\n",
      "Epoch: 75, Loss: 96080500000.0\n",
      "Epoch: 76, Loss: 93902545000.0\n",
      "Epoch: 77, Loss: 91752440000.0\n",
      "Epoch: 78, Loss: 89798880000.0\n",
      "Epoch: 79, Loss: 87810940000.0\n",
      "Epoch: 80, Loss: 85886980000.0\n",
      "Epoch: 81, Loss: 84056320000.0\n",
      "Epoch: 82, Loss: 82179760000.0\n",
      "Epoch: 83, Loss: 80220450000.0\n",
      "Epoch: 84, Loss: 78519470000.0\n",
      "Epoch: 85, Loss: 76927350000.0\n",
      "Epoch: 86, Loss: 75305710000.0\n",
      "Epoch: 87, Loss: 73678995000.0\n",
      "Epoch: 88, Loss: 72149770000.0\n",
      "Epoch: 89, Loss: 70712440000.0\n",
      "Epoch: 90, Loss: 69202730000.0\n",
      "Epoch: 91, Loss: 67740320000.0\n",
      "Epoch: 92, Loss: 66384605000.0\n",
      "Epoch: 93, Loss: 65059190000.0\n",
      "Epoch: 94, Loss: 63739548000.0\n",
      "Epoch: 95, Loss: 62535807000.0\n",
      "Epoch: 96, Loss: 61349536000.0\n",
      "Epoch: 97, Loss: 60230975000.0\n",
      "Epoch: 98, Loss: 59063652000.0\n",
      "Epoch: 99, Loss: 57915410000.0\n",
      "Epoch: 100, Loss: 56786153000.0\n",
      "Epoch: 101, Loss: 55702120000.0\n",
      "Epoch: 102, Loss: 54706123000.0\n",
      "Epoch: 103, Loss: 53678770000.0\n",
      "Epoch: 104, Loss: 52729750000.0\n",
      "Epoch: 105, Loss: 51791560000.0\n",
      "Epoch: 106, Loss: 50831946000.0\n",
      "Epoch: 107, Loss: 49894703000.0\n",
      "Epoch: 108, Loss: 48941730000.0\n",
      "Epoch: 109, Loss: 48039050000.0\n",
      "Epoch: 110, Loss: 47176843000.0\n",
      "Epoch: 111, Loss: 46322240000.0\n",
      "Epoch: 112, Loss: 45515125000.0\n",
      "Epoch: 113, Loss: 44729692000.0\n",
      "Epoch: 114, Loss: 43974648000.0\n",
      "Epoch: 115, Loss: 43206672000.0\n",
      "Epoch: 116, Loss: 42437317000.0\n",
      "Epoch: 117, Loss: 41730950000.0\n",
      "Epoch: 118, Loss: 41054224000.0\n",
      "Epoch: 119, Loss: 40353853000.0\n",
      "Epoch: 120, Loss: 39708320000.0\n",
      "Epoch: 121, Loss: 39073497000.0\n",
      "Epoch: 122, Loss: 38483632000.0\n",
      "Epoch: 123, Loss: 37888660000.0\n",
      "Epoch: 124, Loss: 37297254000.0\n",
      "Epoch: 125, Loss: 36720900000.0\n",
      "Epoch: 126, Loss: 36181450000.0\n",
      "Epoch: 127, Loss: 35650060000.0\n",
      "Epoch: 128, Loss: 35099970000.0\n",
      "Epoch: 129, Loss: 34593554000.0\n",
      "Epoch: 130, Loss: 34075863000.0\n",
      "Epoch: 131, Loss: 33573511000.0\n",
      "Epoch: 132, Loss: 33075155000.0\n",
      "Epoch: 133, Loss: 32580198000.0\n",
      "Epoch: 134, Loss: 32090937000.0\n",
      "Epoch: 135, Loss: 31616666000.0\n",
      "Epoch: 136, Loss: 31176845000.0\n",
      "Epoch: 137, Loss: 30746714000.0\n",
      "Epoch: 138, Loss: 30344305000.0\n",
      "Epoch: 139, Loss: 29941664000.0\n",
      "Epoch: 140, Loss: 29571965000.0\n",
      "Epoch: 141, Loss: 29167065000.0\n",
      "Epoch: 142, Loss: 28817986000.0\n",
      "Epoch: 143, Loss: 28473045000.0\n",
      "Epoch: 144, Loss: 28114833000.0\n",
      "Epoch: 145, Loss: 27751367000.0\n",
      "Epoch: 146, Loss: 27404786000.0\n",
      "Epoch: 147, Loss: 27080070000.0\n",
      "Epoch: 148, Loss: 26750935000.0\n",
      "Epoch: 149, Loss: 26434245000.0\n",
      "Epoch: 150, Loss: 26122783000.0\n",
      "Epoch: 151, Loss: 25809787000.0\n",
      "Epoch: 152, Loss: 25510435000.0\n",
      "Epoch: 153, Loss: 25211042000.0\n",
      "Epoch: 154, Loss: 24916505000.0\n",
      "Epoch: 155, Loss: 24638775000.0\n",
      "Epoch: 156, Loss: 24349977000.0\n",
      "Epoch: 157, Loss: 24055861000.0\n",
      "Epoch: 158, Loss: 23783780000.0\n",
      "Epoch: 159, Loss: 23521020000.0\n",
      "Epoch: 160, Loss: 23271565000.0\n",
      "Epoch: 161, Loss: 23024425000.0\n",
      "Epoch: 162, Loss: 22783525000.0\n",
      "Epoch: 163, Loss: 22549539000.0\n",
      "Epoch: 164, Loss: 22318450000.0\n",
      "Epoch: 165, Loss: 22083020000.0\n",
      "Epoch: 166, Loss: 21859514000.0\n",
      "Epoch: 167, Loss: 21643813000.0\n",
      "Epoch: 168, Loss: 21419176000.0\n",
      "Epoch: 169, Loss: 21200360000.0\n",
      "Epoch: 170, Loss: 21001507000.0\n",
      "Epoch: 171, Loss: 20806484000.0\n",
      "Epoch: 172, Loss: 20619037000.0\n",
      "Epoch: 173, Loss: 20435522000.0\n",
      "Epoch: 174, Loss: 20250847000.0\n",
      "Epoch: 175, Loss: 20082463000.0\n",
      "Epoch: 176, Loss: 19897412000.0\n",
      "Epoch: 177, Loss: 19719200000.0\n",
      "Epoch: 178, Loss: 19537080000.0\n",
      "Epoch: 179, Loss: 19363684000.0\n",
      "Epoch: 180, Loss: 19186459000.0\n",
      "Epoch: 181, Loss: 19011912000.0\n",
      "Epoch: 182, Loss: 18843621000.0\n",
      "Epoch: 183, Loss: 18673046000.0\n",
      "Epoch: 184, Loss: 18509484000.0\n",
      "Epoch: 185, Loss: 18343076000.0\n",
      "Epoch: 186, Loss: 18180125000.0\n",
      "Epoch: 187, Loss: 18032136000.0\n",
      "Epoch: 188, Loss: 17876308000.0\n",
      "Epoch: 189, Loss: 17721380000.0\n",
      "Epoch: 190, Loss: 17567082000.0\n",
      "Epoch: 191, Loss: 17418721000.0\n",
      "Epoch: 192, Loss: 17271206000.0\n",
      "Epoch: 193, Loss: 17121720000.0\n",
      "Epoch: 194, Loss: 16980268000.0\n",
      "Epoch: 195, Loss: 16837751000.0\n",
      "Epoch: 196, Loss: 16700635000.0\n",
      "Epoch: 197, Loss: 16565529000.0\n",
      "Epoch: 198, Loss: 16434380000.0\n",
      "Epoch: 199, Loss: 16305617000.0\n"
     ]
    }
   ],
   "source": [
    "giffy, final_image = NST('./content_images/stonearch_bridge.jpg', \n",
    "                         './style_images/starry_night.jpg', \n",
    "                         epochs = 200, \n",
    "                         steps_per_epoch = 5,\n",
    "                         start_from_content = False,\n",
    "                         style_weight=1e-1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image = img.fromarray((final_image * 255).astype('uint8'), 'RGB')\n",
    "final_image.save('./output_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\n",
    "    img.fromarray((im * 255).astype('uint8'), 'RGB')\n",
    "    for im in giffy if np.sum(im) > 0.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].save('./evolution.gif', save_all=True, append_images=images[1:] + [images[len(images) - 1] for i in range(len(images)//4)], optimize=False, duration=40, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
